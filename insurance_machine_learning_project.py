# -*- coding: utf-8 -*-
"""Insurance Machine learning project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jh11XDufUjjGQztYaltOK7n9DeTaskQV

# importing the dependencies
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn import metrics

"""# Data loading & exploration"""

df=pd.read_csv("insurance.csv")
df.head()

# shape of dataset
df.shape

# getting information about dataset
df.info()

#obtaining the statistical summary
df.describe()

# checking for missing values
df.isnull().sum()



"""Changing categorical features in Numeric feature

**Data** **Analysis**
"""

# distribution of age value
sns.set()
plt.figure(figsize=(4,4))
sns.histplot(df["age"])
plt.title("age_distribution")
plt.show()

# Gender column
plt.figure(figsize=(3,4))
sns.countplot(x="sex", data= df)
plt.title("gender_distribution")
plt.show()

# Gender count
df['sex'].value_counts()

#distribution of 'bmi' column
plt.figure(figsize=(4,4))
sns.histplot(df['bmi'])
plt.show()

plt.figure(figsize=(4,3))
sns.kdeplot(df['bmi'])
plt.show()

#distribution of 'children' column
plt.figure(figsize=(4,4))
sns.countplot(x='children', data=df)
plt.show()

df['children'].value_counts()

#distribution of 'smoker' column
plt.figure(figsize=(3,4))
sns.histplot(x='smoker', data=df)
plt.title("smoker")
plt.show()

df['smoker'].value_counts()

# #distribution of 'region' column
plt.figure(figsize=(5,3.5))
sns.countplot(x='region', data=df)
plt.title("Regional_distribution")
plt.show()

df['region'].value_counts()

#distribution of 'charges' column
plt.figure(figsize=(4,4))
sns.kdeplot(df['charges'])
plt.title("charges_distribution")
plt.show()

plt.figure(figsize=(6,4))
sns.histplot(df['charges'])
plt.title("charges_distribution")
plt.show()

"""Data Pre-Processing

Encoding catogorical features into Numeric features
"""

# encoding 'sex' column
df.replace({'sex':{'male':0,'female':1}}, inplace=True)

3 # encoding 'smoker' column
df.replace({'smoker':{'yes':0,'no':1}}, inplace=True)

# encoding 'region' column
df.replace({'region':{'southeast':0,'southwest':1,'northeast':2,'northwest':3}}, inplace=True)

df.tail()

#correlation Analysis of features
correlation_matrix=df.corr()
correlation_matrix

# Heat_map(how effeicient the correlation is)
sns.heatmap(correlation_matrix,annot=True, cmap="coolwarm", linewidths=.5)

"""Splitting the Features and Target

"""

X = df.drop('charges', axis=1)
y= df['charges']

X.head()

y.head()

"""Splitting the data into Training data & Testing Data"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=52)

print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

"""Model Training

1.Linear Regression
"""

# loading the Linear Regression model
model = LinearRegression()

model.fit(X_train, y_train)

"""Model Evaluation"""

# prediction on training data
y_td_predict =model.predict(X_train)

# R2 value for trainig data
r2_train = metrics.r2_score(y_train, y_td_predict)
print('R squared value : ', r2_train)

# prediction on test data
y_test_pred = model.predict(X_test)

# R2 value for test data
r2_test= metrics.r2_score(y_test, y_test_pred)
print('R squared value : ', r2_test)

"""2. Random Forest Regressor"""

# importing Random forest Regressor
from sklearn.ensemble import RandomForestRegressor

# model initiation
model = RandomForestRegressor()

#model training
model.fit(X_train, y_train)

# prediction on Trining data
y_tdr_predict =model.predict(X_train)

# R2 value for trainig data
r2_train_rand = metrics.r2_score(y_train, y_tdr_predict)
print('R squared value : ', r2_train_rand)

# prediction on test data
y_pred = model.predict(X_test)

# R2 value for test data
r2_test_rand= metrics.r2_score(y_test, y_pred)
print('R squared value : ', r2_test_rand)

"""significat difference in both r2 values for training and test data respectively (Overfitting)"""

# applying pruning technique to prevent overfitting
model_pruned = RandomForestRegressor(max_depth=3)
model_pruned.fit(X_train, y_train)

y_tdr_predict = model_pruned.predict(X_train)

# R2 value for trainig data
r2_train_rand_pruned = metrics.r2_score(y_train, y_tdr_predict)
print('R squared value : ', r2_train_rand_pruned)

y_pred_pruned = model_pruned.predict(X_test)

r2_test_pruned = metrics.r2_score(y_test, y_pred_pruned)
print('R squared value : ', r2_test_pruned)

"""Clearly the technique is helping in preveting overfitting.

3. XGBoost regressor
"""

import xgboost as xgb

# Convert data into DMatrix format (optimized data structure for XGBoost)
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# Define the XGBoost parameters
params = {
    'objective': 'reg:squarederror',  # For regression tasks
    'max_depth': 3,
    'learning_rate': 0.1,
}

# Train the XGBoost model
model = xgb.train(params, dtrain, num_boost_round=30)

# Make predictions on the train set
y_train_pred = model.predict(dtrain)

# R2 value for train data
r2_train_xgb = metrics.r2_score(y_train, y_train_pred)
print('R squared value : ', r2_train_xgb)

# Make predictions on the test set
y_pred = model.predict(dtest)

# R2 value for test data
r2_test_xgb = metrics.r2_score(y_test, y_pred)
print('R squared value : ', r2_test_xgb)

"""Clearly simple Linear regresson model gives less generalised model as compared to complex Random forest or xgboost model.
depending upon various factors, we need to select best model for our task.

"""